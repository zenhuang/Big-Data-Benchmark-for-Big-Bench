#!/usr/bin/env bash

helpModule () {
  echo "This module generates a dataset with PDGF using hadoop"
  echo
  echo "Options:"
  echo -e "-f\tscale factor for PDGF"
  echo -e "-h\tshow this help"
  echo -e "-m\tmap tasks for PDGF"
}

runModule () {
  # make sure that this method stops when an error occured
  BIG_BENCH_STOP_AFTER_FAILURE="1"

  if [ -z "$BIG_BENCH_MAP_TASKS" ]
  then
    echo "The number of map tasks is a required option"
    return 1
  fi

  local HadoopClusterExecOptions="-mapTasks $BIG_BENCH_MAP_TASKS"

  if [ -n "$BIG_BENCH_SCALE_FACTOR" ]
  then
    local PDGF_OPTIONS="-sf $BIG_BENCH_SCALE_FACTOR"
  fi

  echo "PDGFOptions: $PDGF_OPTIONS $@"
  echo "HadoopClusterExecOptions: $HadoopClusterExecOptions"

  local PDGF_ARCHIVE_NAME="pdgfEnvironment.tar"
  local PDGF_DISTRIBUTED_NODE_DIR="$PDGF_ARCHIVE_NAME/data-generator/"
  local PDGF_ARCHIVE_PATH="`mktemp -d`/$PDGF_ARCHIVE_NAME"

  if grep -q "IS_EULA_ACCEPTED=true" "$BIG_BENCH_DATA_GENERATOR_DIR/Constants.properties"
  then
    echo "EULA is accepted"
  else
    echo "==============================================="
    echo "data generator EULA"
    echo "==============================================="
    echo "This is your first run of the data generation tool. Please accept the EULA."
    java -jar "$BIG_BENCH_DATA_GENERATOR_DIR/pdgf.jar" -ns -c
    if grep -q "IS_EULA_ACCEPTED=true" "$BIG_BENCH_DATA_GENERATOR_DIR/Constants.properties"
    then
      echo "OK"
    else
      echo "ERROR! data generation tool EULA is not accepted. Cannot procced"
      return 1 
    fi
  fi

  # delete any previously generated data
  echo "==============================================="
  echo "Deleting any previously generated data."
  echo "==============================================="
  #runCmdWithErrorCheck "${BIG_BENCH_BIN_DIR}/bigBench" cleanData $LIST_OF_USER_OPTIONS
  "${BIG_BENCH_BIN_DIR}/bigBench" cleanData $LIST_OF_USER_OPTIONS
  echo "OK"
  echo "==============================================="
  echo "make hdfs benchmark data dir: "${BIG_BENCH_HDFS_ABSOLUTE_INIT_DATA_DIR}
  echo "==============================================="
  runCmdWithErrorCheck hadoop fs -mkdir -p "${BIG_BENCH_HDFS_ABSOLUTE_INIT_DATA_DIR}"
  runCmdWithErrorCheck hadoop fs -chmod -R 777 "${BIG_BENCH_HDFS_ABSOLUTE_INIT_DATA_DIR}"

  hadoop fs -ls "${BIG_BENCH_HDFS_ABSOLUTE_INIT_DATA_DIR}"

  echo "OK"
  echo "==============================================="
  echo "make hdfs benchmark data dir: "${BIG_BENCH_HDFS_ABSOLUTE_REFRESH_DATA_DIR}
  echo "==============================================="
  runCmdWithErrorCheck hadoop fs -mkdir -p "${BIG_BENCH_HDFS_ABSOLUTE_REFRESH_DATA_DIR}"
  runCmdWithErrorCheck hadoop fs -chmod -R 777 "${BIG_BENCH_HDFS_ABSOLUTE_REFRESH_DATA_DIR}"

  hadoop fs -ls "${BIG_BENCH_HDFS_ABSOLUTE_REFRESH_DATA_DIR}"
  echo "OK"


  echo "==============================================="
  echo "Creating data generator archive to upload to DistCache"
  echo "==============================================="
  echo "creating: $PDGF_ARCHIVE_PATH"
  rm -f "$PDGF_ARCHIVE_PATH"
  runCmdWithErrorCheck tar -C "$BIG_BENCH_HOME" -caf "$PDGF_ARCHIVE_PATH" data-generator/
  echo "OK"

  echo "==============================================="
  echo "Starting distributed hadoop data generation job with: $HadoopClusterExecOptions"
  echo "Temporary result data in hdfs: ${BIG_BENCH_HDFS_ABSOLUTE_INIT_DATA_DIR} (you can change the data generation target folder in  the /setEnvVars configuration file with the BIG_BENCH_HDFS_ABSOLUTE_INIT_DATA_DIR property)"
  echo "logs: ${BIG_BENCH_DATAGEN_STAGE_LOG}"
  echo "==============================================="
  local HADOOP_CP=`hadoop classpath`
  echo "HADOOP CLASSPATH: "$HADOOP_CP

  local PDGF_CLUSTER_CONF="-Dpdgf.log.folder=/tmp/pdgfLog/HadoopClusterExec.taskNumber -Dcore-site.xml=${BIG_BENCH_DATAGEN_CORE_SITE} -Dhdfs-site.xml=${BIG_BENCH_DATAGEN_HDFS_SITE} -Djava.library.path=${BIG_BENCH_HADOOP_LIBS_NATIVE} -DFileChannelProvider=pdgf.util.caching.fileWriter.HDFSChannelProvider -Ddfs.replication.override=${BIG_BENCH_DATAGEN_DFS_REPLICATION} "
  echo "PDGF_CLUSTER_CONF: $PDGF_CLUSTER_CONF"

  echo "create $BIG_BENCH_LOGS_DIR folder"
  mkdir -p "$BIG_BENCH_LOGS_DIR"

  echo hadoop jar "${BIG_BENCH_TOOLS_DIR}/HadoopClusterExec.jar" -archives "${PDGF_ARCHIVE_PATH}" ${BIG_BENCH_DATAGEN_HADOOP_EXEC_DEBUG} -taskFailOnNonZeroReturnValue -execCWD "${PDGF_DISTRIBUTED_NODE_DIR}" ${HadoopClusterExecOptions} -exec ${BIG_BENCH_DATAGEN_HADOOP_JVM_ENV} -cp "${HADOOP_CP}:pdgf.jar" ${PDGF_CLUSTER_CONF} pdgf.Controller -nc HadoopClusterExec.tasks -nn HadoopClusterExec.taskNumber -ns -c -sp REFRESH_PHASE 0 -o "'${BIG_BENCH_HDFS_ABSOLUTE_INIT_DATA_DIR}/'+table.getName()+'/'" ${BIG_BENCH_DATAGEN_HADOOP_OPTIONS} -s ${BIG_BENCH_DATAGEN_TABLES} ${PDGF_OPTIONS} "$@"

  TIME_MEASUREMENT_FILE="`mktemp`"
  echo "======= Generating base data time ============" > "$TIME_MEASUREMENT_FILE"
  { time runCmdWithErrorCheck hadoop jar "${BIG_BENCH_TOOLS_DIR}/HadoopClusterExec.jar" -archives "${PDGF_ARCHIVE_PATH}" ${BIG_BENCH_DATAGEN_HADOOP_EXEC_DEBUG} -taskFailOnNonZeroReturnValue -execCWD "${PDGF_DISTRIBUTED_NODE_DIR}" ${HadoopClusterExecOptions} -exec ${BIG_BENCH_DATAGEN_HADOOP_JVM_ENV} -cp "${HADOOP_CP}:pdgf.jar" ${PDGF_CLUSTER_CONF} pdgf.Controller -nc HadoopClusterExec.tasks -nn HadoopClusterExec.taskNumber -ns -c -sp REFRESH_PHASE 0 -o "'${BIG_BENCH_HDFS_ABSOLUTE_INIT_DATA_DIR}/'+table.getName()+'/'" ${BIG_BENCH_DATAGEN_HADOOP_OPTIONS} -s ${BIG_BENCH_DATAGEN_TABLES} ${PDGF_OPTIONS} "$@" > >(tee -a "$BIG_BENCH_DATAGEN_STAGE_LOG") 2>&1 ; } 2>> "$TIME_MEASUREMENT_FILE"
  cat "$TIME_MEASUREMENT_FILE" >> "$BIG_BENCH_DATAGEN_STAGE_LOG"

  echo hadoop jar "${BIG_BENCH_TOOLS_DIR}/HadoopClusterExec.jar" -archives "${PDGF_ARCHIVE_PATH}" ${BIG_BENCH_DATAGEN_HADOOP_EXEC_DEBUG} -taskFailOnNonZeroReturnValue -execCWD "${PDGF_DISTRIBUTED_NODE_DIR}" ${HadoopClusterExecOptions} -exec ${BIG_BENCH_DATAGEN_HADOOP_JVM_ENV} -cp "${HADOOP_CP}:pdgf.jar" ${PDGF_CLUSTER_CONF} pdgf.Controller -nc HadoopClusterExec.tasks -nn HadoopClusterExec.taskNumber -ns -c -sp REFRESH_PHASE 1 -o "'${BIG_BENCH_HDFS_ABSOLUTE_REFRESH_DATA_DIR}/'+table.getName()+'/'" ${BIG_BENCH_DATAGEN_HADOOP_OPTIONS} -s ${BIG_BENCH_DATAGEN_TABLES} ${PDGF_OPTIONS} "$@"

  echo "======= Generating refresh data time =========" > "$TIME_MEASUREMENT_FILE"
  { time runCmdWithErrorCheck hadoop jar "${BIG_BENCH_TOOLS_DIR}/HadoopClusterExec.jar" -archives "${PDGF_ARCHIVE_PATH}" ${BIG_BENCH_DATAGEN_HADOOP_EXEC_DEBUG} -taskFailOnNonZeroReturnValue -execCWD "${PDGF_DISTRIBUTED_NODE_DIR}" ${HadoopClusterExecOptions} -exec ${BIG_BENCH_DATAGEN_HADOOP_JVM_ENV} -cp "${HADOOP_CP}:pdgf.jar" ${PDGF_CLUSTER_CONF} pdgf.Controller -nc HadoopClusterExec.tasks -nn HadoopClusterExec.taskNumber -ns -c -sp REFRESH_PHASE 1 -o "'${BIG_BENCH_HDFS_ABSOLUTE_REFRESH_DATA_DIR}/'+table.getName()+'/'" ${BIG_BENCH_DATAGEN_HADOOP_OPTIONS} -s ${BIG_BENCH_DATAGEN_TABLES} ${PDGF_OPTIONS} "$@" > >(tee -a "$BIG_BENCH_DATAGEN_STAGE_LOG") 2>&1 ; } 2>> "$TIME_MEASUREMENT_FILE"
  cat "$TIME_MEASUREMENT_FILE" >> "$BIG_BENCH_DATAGEN_STAGE_LOG"

  ##cleanup
  rm -rf "$TIME_MEASUREMENT_FILE"
  rm -f ${PDGF_ARCHIVE_PATH}

  echo "==============================================="
  echo "Verify data sizes"
  echo "==============================================="
  hadoop fs -du -h "${BIG_BENCH_HDFS_ABSOLUTE_INIT_DATA_DIR}"
  hadoop fs -du -h "${BIG_BENCH_HDFS_ABSOLUTE_REFRESH_DATA_DIR}"

  echo "==============================================="
  echo "Hadoop data generation job finished. "
  echo "logs: ${BIG_BENCH_DATAGEN_STAGE_LOG}"
  echo "View generated files: hadoop fs -ls ${BIG_BENCH_HDFS_ABSOLUTE_INIT_DATA_DIR}"
  echo "View generated refresh files: hadoop fs -ls ${BIG_BENCH_HDFS_ABSOLUTE_REFRESH_DATA_DIR}"
  echo "==============================================="
}
