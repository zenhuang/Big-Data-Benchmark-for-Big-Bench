<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="chrome=1">
    <title>Big-bench by intel-hadoop</title>

    <link rel="stylesheet" href="stylesheets/styles.css">
    <link rel="stylesheet" href="stylesheets/pygment_trac.css">
    <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
    <!--[if lt IE 9]>
    <script src="//html5shiv.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->
  </head>
  <body>
    <div class="wrapper">
      <header>
        <h1>Big-bench</h1>
        <p>BigBench -- Big Data Analytics Hadoop Benchmark.</p>

        <p class="view"><a href="https://github.com/intel-hadoop/Big-Bench">View the Project on GitHub <small>intel-hadoop/Big-Bench</small></a></p>


        <ul>
          <li><a href="https://github.com/intel-hadoop/Big-Bench/zipball/master">Download <strong>ZIP File</strong></a></li>
          <li><a href="https://github.com/intel-hadoop/Big-Bench/tarball/master">Download <strong>TAR Ball</strong></a></li>
          <li><a href="https://github.com/intel-hadoop/Big-Bench">View On <strong>GitHub</strong></a></li>
        </ul>
      </header>
      <section>
        <p>Welcome to BigBench -- BigBench is a proposed Industry standard Big Data Batch analytics benchmark. This implementation of BigBench is powered by Hadoop Mapreduce, HIVE, Mahout for data processing and HDFS for data storage. </p>

<p>This wiki will be primary source of BigBench documentation for users to download and run the benchmark on their Hadoop clusters. </p>

<p>The Benchmark kit offered here is functional and has been extensively tested, however  and updating the code so please consider the benchmark kit as under development. </p>

<p>Post your questions on BigBench Google group.</p>

<p><a href="https://groups.google.com/forum/#!forum/big-bench">https://groups.google.com/forum/#!forum/big-bench</a> </p>

<p>UNDER DEVELOPMENT -- Post your questions on Google Groups <a href="https://groups.google.com/forum/#!forum/big-bench">https://groups.google.com/forum/#!forum/big-bench</a>
 for help any help in running the workload.</p>

<p>To collect performance metrics from  Hadoop nodes and analyze the resource utilization draw automated charts using MS-Excel, PAT is available for download.</p>

<p><a href="https://github.com/intel-hadoop/PAT">https://github.com/intel-hadoop/PAT</a> </p>

<h1></h1>

<p>This document is a development version and describes the BigBench installation and execution on our AWS machines.</p>

<h1>
<a name="preparation" class="anchor" href="#preparation"><span class="octicon octicon-link"></span></a>Preparation</h1>

<h2>
<a name="cluster-environment" class="anchor" href="#cluster-environment"><span class="octicon octicon-link"></span></a>Cluster Environment</h2>

<p><strong>Java</strong></p>

<p>Java 1.7 is required. 64 bit is recommended</p>

<p><strong>Hadoop</strong></p>

<ul>
<li>Hive 0.12 recommended</li>
<li>Mahout</li>
</ul><h2>
<a name="installation" class="anchor" href="#installation"><span class="octicon octicon-link"></span></a>Installation</h2>

<p>On the AWS installation, clone the github repository into a folder stored in $INSTALL_DIR:</p>

<pre><code>export INSTALL_DIR="$HOME" # adapt this to your location
cd $INSTALL_DIR
git clone https://&lt;username&gt;@github.com/intel-hadoop/Big-Bench.git
</code></pre>

<h2>
<a name="configuration" class="anchor" href="#configuration"><span class="octicon octicon-link"></span></a>Configuration</h2>

<p>Check if the hadoop related variables are correctly set in the environment file:</p>

<p><code>vi "$INSTALL_DIR/Big-Bench/setEnvVars"</code></p>

<p>Major settings, Specify your cluster environment:</p>

<pre><code>BIG_BENCH_HADOOP_LIBS_NATIVE  (optional but speeds up hdfs access)
BIG_BENCH_HADOOP_CONF         most important: core-site.xml and hdfs-site.xml
</code></pre>

<p>Minor settings:</p>

<pre><code>BIG_BENCH_USER
BIG_BENCH_DATAGEN_DFS_REPLICATION  replication count used during generation of the big bench table
BIG_BENCH_DATAGEN_JVM_ENV          -Xmx750m is sufficient for Nodes with 2 CPU cores, remove or increase if your Nodes have more cores
</code></pre>

<h1>
<a name="run-the-workload" class="anchor" href="#run-the-workload"><span class="octicon octicon-link"></span></a>Run the workload</h1>

<p>There are two different methods for running the workload: use the driver to simply perform a complete benchmark run or use the bash scripts to do partial tests. As the driver calls the bash scripts internally, both methods yield the same results.</p>

<h2>
<a name="common-hints" class="anchor" href="#common-hints"><span class="octicon octicon-link"></span></a>Common hints</h2>

<p>The following paragraphs are important for both methods.</p>

<h3>
<a name="accept-license" class="anchor" href="#accept-license"><span class="octicon octicon-link"></span></a>Accept license</h3>

<p>When running the data generator for the first time, the user must accept its license:</p>

<pre><code>By using this software you must first agree to our terms of use. Press [ENTER] to show them
... # license is displayed
If you have read and agree to these terms of use, please type (uppercase!): YES and press [ENTER]
YES
</code></pre>

<h3>
<a name="pdgf" class="anchor" href="#pdgf"><span class="octicon octicon-link"></span></a>PDGF</h3>

<p>The data are being generated directly into HDFS (into the $BIG_BENCH_HDFS_RELATIVE_INIT_DATA_DIR directory, absolute HDFS path is $BIG_BENCH_HDFS_ABSOLUTE_INIT_DATA_DIR).</p>

<p>Default HDFS replication count is 1 (data is only stored on the generating node). You can change this in the $BIG_BENCH_HOME/setEnvVars file by changing the variable
`BIG_BENCH_DATAGEN_DFS_REPLICATION='.</p>

<h2>
<a name="using-the-bigbench-driver" class="anchor" href="#using-the-bigbench-driver"><span class="octicon octicon-link"></span></a>Using the BigBench driver</h2>

<p>The BigBench driver is started with a script. To show all available options, you can call the help first:</p>

<pre><code>"$INSTALL_DIR/scripts/bigBench runBenchmark -h
</code></pre>

<h3>
<a name="quick-start" class="anchor" href="#quick-start"><span class="octicon octicon-link"></span></a>Quick start</h3>

<p>If a complete benchmark run should be performed and no data were generated previously, this is the command which should be executed:</p>

<pre><code>"$INSTALL_DIR/scripts/bigBench runBenchmark -m &lt;number of map tasks for data generation&gt; -f &lt;scale factor of dataset&gt; -s &lt;number of parallel streams in the throughput test&gt;
</code></pre>

<p>This command will generate data, run the load-, power- and throughput-test and calculate the BigBench result.</p>

<p>So a complete benchmark run with all stages can be done by running (e.g., 4 map tasks, scale factor 100, 2 streams):</p>

<pre><code>"$INSTALL_DIR/scripts/bigBench runBenchmark -m 4 -f 100 -s 2
</code></pre>

<p>After the benchmark finished, two log files are written: BigBenchResult.txt (which contains the driver's sysout messages) as well as BigBenchTimes.csv (which contains all measured timestamps/durations). The log directory can be specified with the -l option, it defaults to the BigBench's log dir ($BIG_BENCH_LOGS_DIR).</p>

<h3>
<a name="more-detailed-explanation" class="anchor" href="#more-detailed-explanation"><span class="octicon octicon-link"></span></a>More detailed explanation</h3>

<p>There are four phases the driver traverses (only three are benchmarked though): data generation, load test, power test and throughput test. The driver has a clean option (-c) which does not run the benchmark but rather cleans the environment from previous runs (if for some reason all generated data should be cleaned).</p>

<h4>
<a name="data-generation" class="anchor" href="#data-generation"><span class="octicon octicon-link"></span></a>Data generation</h4>

<p>The data generation phase is not benchmarked by BigBench. The driver can skip this phase by setting "-sd". Skipping this phase is a good idea if data were already generated previously and the complete benchmark should be repeated with the same dataset size. In that case, generting data is not necessary as PDGF would generate the exact same data as in the previous run. If data generation is not skipped, two other options must be provided to the driver: "-m" sets the number of map tasks for PDGF's data generation, "-f" sets the scale factor determining the dataset size (1 scale factor equals 1 GiB). If "-c" is set and "-sd" is not set, the dataset directory in HDFS will be deleted.</p>

<h4>
<a name="load-test" class="anchor" href="#load-test"><span class="octicon octicon-link"></span></a>Load test</h4>

<p>Population of the hive metastore is the first phase that is benchmarked by BigBench. This phase can be skipped by providing "-sl" as an option. Re-populating the metastore is technically only necessary if the dataset has changed. Nevertheless, metastore population is part of the benchmark, so if this phase is skipped then no BigBench result can be computed. If "-c" is set and "-sl" is not set, the tables of the dataset in the metastore will be dropped (however it does not influence the query results from later tests).</p>

<h4>
<a name="power-test" class="anchor" href="#power-test"><span class="octicon octicon-link"></span></a>Power test</h4>

<p>This is the second phase that is benchmarked by BigBench. All queries run sequentially in one stream. The phase can be skipped with the option "-sp". Setting "-c" (and not "-sp") cleans previous power-test's results in the hive metastore tables and HDFS directories.</p>

<h4>
<a name="throughput-test" class="anchor" href="#throughput-test"><span class="octicon octicon-link"></span></a>Throughput test</h4>

<p>The throughput test is the last benchmark phase. All queries run in parallel streams in different order. The phase can be skipped with "-st". If this phase is not skipped, "-s" is a required option because that sets the number of parallel streams used in this phase. As in the other phases, setting "-c" (and not "-st") cleans the thoughput-test's results in the hive metastore and the HDFS directories.</p>

<h2>
<a name="using-the-bigbench-bash-script" class="anchor" href="#using-the-bigbench-bash-script"><span class="octicon octicon-link"></span></a>Using the bigBench bash script</h2>

<p>The driver internally calls the $BIG_BENCH_BASH_SCRIPT_DIR/bigBench bash script along with a module name. So every step the driver performs (apart from the more complicated "query mixing" and multi-stream execution logic) can be run manually by executing this script with the proper options.</p>

<h3>
<a name="overview" class="anchor" href="#overview"><span class="octicon octicon-link"></span></a>Overview</h3>

<p>The general syntax for the bigBench script is:</p>

<pre><code>"$INSTALL_DIR/scripts/bigBench [global options] moduleName [module options]
</code></pre>

<p>At the moment there is only one module which processes module options itself, namely runBenchmark. All other modules currently do NO option processing. They rely on bigBench for option processing. Therefore when not running the runBenchmark module, global options must be specified.</p>

<p>All available options as well as all found modules can be listed by calling the script help:</p>

<pre><code>"$INSTALL_DIR/scripts/bigBench -h
</code></pre>

<h3>
<a name="available-options" class="anchor" href="#available-options"><span class="octicon octicon-link"></span></a>Available options</h3>

<ul>
<li>-b: This option chooses which binary will be used for the benchmark. WARNING: support for choosing other binaries than "hive" is implemented, but hive is the only tested binary. In fact, no other binary works so far. DO NOT USE THAT OPTION</li>
<li>-d: Some more complex queries are split into multiple internal parts. This option chooses which internal query part will be executed. This is a developer only option. ONLY USE IF YOU KNOW WHAT YOU ARE DOING</li>
<li>-f: The scale factor for PDGF. It is used by the clusterDataGen and hadoopDataGen modules</li>
<li>-h: Show help</li>
<li>-m: The map tasks used for data generation. It is used by the clusterDataGen and hadoopDataGen modules</li>
<li>-p: The benchmark phase to use. It is necessary if subsequent query runs should not overwrite results of previous queries. The driver internally uses POWER_TEST_IN_PROGRESS, THROUGHPUT_TEST_FIRST_QUERY_RUN_IN_PROGRESS and THROUGHPUT_TEST_SECOND_QUERY_RUN_IN_PROGRESS. The default value when not providing this option is RUN_QUERY</li>
<li>-q: Defines the query number to be executed</li>
<li>-s: This option defines the number of parallel streams to use. It is only of any use with the runQueryInParallel module</li>
<li>-t: Sets the stream number of the current query. This option is important so that one query can run multiple times in parallel without interfering with other instances</li>
<li>-v: Use the provided file as initial metastore population script</li>
<li>-w: Use the provided file as metastore refresh script</li>
<li>-y: Use the provided file for custom query parameters</li>
<li>-z: Use the provided file for custom hive settings</li>
</ul><h3>
<a name="modules-usage-examples" class="anchor" href="#modules-usage-examples"><span class="octicon octicon-link"></span></a>Modules usage examples</h3>

<ul>
<li>cleanData: cleans the dataset directory in HDFS. This module is automatically run by the data generator module to remove the dataset from the HDFS.</li>
</ul><pre><code>"$INSTALL_DIR/scripts/bigBench cleanData
</code></pre>

<ul>
<li>cleanMetastore: cleans the metastore dataset tables.</li>
</ul><pre><code>"$INSTALL_DIR/scripts/bigBench [-z &lt;hive settings&gt;] cleanMetastore
</code></pre>

<ul>
<li>cleanQueries: cleans all metastore tables and result directories in HDFS for all 30 queries. This module works as a wrapper for cleanQuery and does not work if "-q" is set as option.</li>
</ul><pre><code>"$INSTALL_DIR/scripts/bigBench [-p &lt;benchmark phase&gt;] [-t &lt;stream number] [-z &lt;hive settings&gt;] cleanQueries
</code></pre>

<ul>
<li>cleanQuery: cleans metastore tables and result directories in HDFS for one query. Needs the query number to be set.</li>
</ul><pre><code>"$INSTALL_DIR/scripts/bigBench [-p &lt;benchmark phase&gt;] -q &lt;query number&gt; [-t &lt;stream number] [-z &lt;hive settings&gt;] cleanQuery
</code></pre>

<ul>
<li><p>clusterDataGen: generates data using ssh on all defined nodes. This module is deprecated. Do not use it.</p></li>
<li><p>hadoopDataGen: generates data using a hadoop job. Needs the map tasks and scale factor options.</p></li>
</ul><pre><code>"$INSTALL_DIR/scripts/bigBench -m &lt;map tasks&gt; -f &lt;scale factor&gt; hadoopDataGen
</code></pre>

<ul>
<li>populateMetastore: populates the metastore with the dataset tables.</li>
</ul><pre><code>"$INSTALL_DIR/scripts/bigBench [-v &lt;population script&gt;] [-z &lt;hive settings&gt;] populateMetastore
</code></pre>

<ul>
<li>refreshMetastore: refreshes the metastore with the refresh dataset.</li>
</ul><pre><code>"$INSTALL_DIR/scripts/bigBench [-w &lt;refresh script&gt;] [-z &lt;hive settings&gt;] refreshMetastore
</code></pre>

<ul>
<li>runBenchmark: runs the driver. This module parses its options itself. For details look at the driver usage section above.</li>
</ul><pre><code>"$INSTALL_DIR/scripts/bigBench runBenchmark [driver options]
</code></pre>

<ul>
<li>runQueries: runs all 30 queries sequentially. This module works as a wrapper for runQuery and does not work if "-q" is set as option.</li>
</ul><pre><code>"$INSTALL_DIR/scripts/bigBench [-p &lt;benchmark phase&gt;] [-t &lt;stream number] [-z &lt;hive settings&gt;] runQueries
</code></pre>

<ul>
<li>runQuery: runs one query. Needs the query number to be set.</li>
</ul><pre><code>"$INSTALL_DIR/scripts/bigBench [-p &lt;benchmark phase&gt;] -q &lt;query number&gt; [-t &lt;stream number] [-z &lt;hive settings&gt;] runQuery
</code></pre>

<ul>
<li>runQueryInParallel: runs one query on multiple parallel streams. This module is a wrapper for runQuery. Needs the query number ("-q") and total number of streams ("-s") to be set and the stream number ("-t") to be unset.</li>
</ul><pre><code>"$INSTALL_DIR/scripts/bigBench [-p &lt;benchmark phase&gt;] -q &lt;query number&gt; -s &lt;number of parallel streams&gt; [-z &lt;hive settings&gt;] runQueryInParallel
</code></pre>

<ul>
<li>showErrors: parses query errors in the log files after query runs.</li>
</ul><pre><code>"$INSTALL_DIR/scripts/bigBench showErrors
</code></pre>

<ul>
<li>showTimes: parses execution times in the log files after query runs.</li>
</ul><pre><code>"$INSTALL_DIR/scripts/bigBench showTimes
</code></pre>

<ul>
<li>zipQueryLogs: generates a zip file of all logs in the logs directory. It is run by the driver after each complete benchmark run. Subsequent runs override the old log files. A zip archive is created to save them before being overwritten.</li>
</ul><pre><code>"$INSTALL_DIR/scripts/bigBench zipQueryLogs
</code></pre>

<h1>
<a name="faq" class="anchor" href="#faq"><span class="octicon octicon-link"></span></a>FAQ</h1>

<p>This Benchmark does not favour any platform and we ran this benchmark on many different distributions. But you gotta start somewhere.
This Benchmark is also not HIVE specify, but hive happens to be the first module to be implemented.</p>

<p>This FAQ is mostly based on our experiments with Hive on Yarn with CDH 5.x)</p>

<h2>
<a name="where-do-i-put-my-cluster-specific-settings" class="anchor" href="#where-do-i-put-my-cluster-specific-settings"><span class="octicon octicon-link"></span></a>Where do i put my cluster specific settings?</h2>

<h1></h1>

<p>Here: Big-Bench/setEnvVars</p>

<p>Where is my core-site.xml/hdfs-site.xml  for BIG_BENCH_HADOOP_CONF (usually the one in /etc/hadoop/...):</p>

<p><code>find / -name "hdfs-site.xml" 2&gt; /dev/null</code></p>

<p>Where is my hdfs native libs folder for BIG_BENCH_HADOOP_LIBS_NATIVE?</p>

<p><code>find / -name "libhadoop.so" 2&gt; /dev/null</code></p>

<p>What is my name node address for BIG_BENCH_HDFS_NAMENODE? </p>

<p>Look inside your hdfs-site.xml and locate this property value:</p>

<pre><code>&lt;property&gt;
    &lt;name&gt;dfs.namenode.servicerpc-address&lt;/name&gt;
    &lt;value&gt;host.domain:8022&lt;/value&gt;
&lt;/property&gt;
</code></pre>

<h2>
<a name="where-do-i-put-benchmark-specific-hive-options" class="anchor" href="#where-do-i-put-benchmark-specific-hive-options"><span class="octicon octicon-link"></span></a>Where do i put benchmark specific hive options?</h2>

<p>Big-Bench/hive/hiveSettings.sql</p>

<p>There are already a number of documented settings in there.</p>

<h2>
<a name="where-do-i-put-query-specific-hive-options" class="anchor" href="#where-do-i-put-query-specific-hive-options"><span class="octicon octicon-link"></span></a>Where do i put query specific hive options?</h2>

<p>You can place an optional file "hiveLocalSettings.sql" into a queries folder e.g.:</p>

<p>Big-Bench/queries/q??/hiveLocalSettings.sql</p>

<p>You can put your query specific settings into this file, and the benchmark will automatically load the file. Its made so, that the hiveLocalSettings.sql file gets loaded last, which allows you to override any previously made settings made in e.g.: Big-Bench/hive/hiveSettings.sql
This way your settings are independent from github updates and there wont be any conflicts when updating the query files. </p>

<h2>
<a name="underutilized-cluster" class="anchor" href="#underutilized-cluster"><span class="octicon octicon-link"></span></a>Underutilized cluster</h2>

<h3>
<a name="cluster-setup" class="anchor" href="#cluster-setup"><span class="octicon octicon-link"></span></a>cluster setup</h3>

<p>Before "tuning" or asking in the google group, please ensure that your cluster is well configured and able to utilize all resources (cpu/mem/storage/netIO).</p>

<p>There are a lot of things you have to configure, depending on your hadoop distribution and your hardware.
Some important variables regarding MapReduce task performance:</p>

<pre><code>mapreduce.reduce.memory.mb
mapreduce.map.memory.mb
mapreduce.map.memory.mb
mapreduce.map.memory.mb
mapreduce.reduce.java.opts;
mapreduce.map.java.opts
mapreduce.map.java.opts
mapreduce.task.io.sort.mb
mapreduce.task.io.sort.mb
...
</code></pre>

<p>Basically, you may want to have at least as much (yarn) "containers" (container may hold a map or a reduce task) on your cluster as you have CPU cores or hardware threads.
Despite that, you configure your container count based on available memory in your cluster. 1-2GB of memory per container may be a good starting point.</p>

<p>In CDH you can do this with: (just example values! follow a more sophisticated tutorial on how to set up your cluster!):</p>

<p><strong>Gateway</strong>
Gateway BaseGroup --expand--&gt; Resource management</p>

<pre><code>Container_Size (e.g.:  1,5Gb can be sufficient but you may require more if you run into "OutOfMemory" or "GC overhead exceeded" errors while executing this benchmark) 
mapreduce.map.memory.mb=Container_Size
mapreduce.reduce.memory.mb=Container_Size
mapreduce.map.java.opts.max.heap =0.75*Container_Size
mapreduce.reduce.java.opts.max.heap =0.75*Container_Size
Client Java Heap Size in Bytes =0.75*Container_Size
</code></pre>

<p><strong>Nodemanager</strong>
Nodemanager BaseGroup --expand--&gt; Resource management</p>

<pre><code> -container memory
 how many memory ,all containers together, can allocate (physical "free" resources on nodes)
 - yarn.nodemanager.resource.cpu-vcores (same rules as container memory)
</code></pre>

<p><strong>ResourceManager</strong>
ResourceManager BaseGroup --expand--&gt; Resource management</p>

<pre><code> -yarn.scheduler.minimum-allocation-mb  (set to 512mb or 1GB)
 -yarn.scheduler.maximum-allocation-mb  (hint: container memory/container max mem == minimum amount of containers per node)
 -yarn.scheduler.increment-allocation-mb  set to 512MB
 -yarn.scheduler.maximum-allocation-vcores  set to min amount of containers
</code></pre>

<p><strong>Dynamic resource pools</strong>
(cluster -&gt; dynamic resource pools -&gt; configuration)</p>

<pre><code>If everything runs fine, do not set anything here (no additional restrictions). 
If you experience yarn deadlocks (yarn trying to allocate resources, but fails leading to MR-jobs waiting indefinitely for containers)  you may want set a limit.
</code></pre>

<h3>
<a name="datagen-stage-tuning-the-datageneration-tool" class="anchor" href="#datagen-stage-tuning-the-datageneration-tool"><span class="octicon octicon-link"></span></a>datagen stage: Tuning the DataGeneration tool</h3>

<p><strong>right settings for number of map tasks (bigBench -m option)</strong></p>

<p>Short answer:
On map task per virtual CPU/hardware thread is best to utilize all CPU resources.</p>

<p>But settings in your cluster may not allow you executing this number of map tasks in parallel. Basically you can not run more parallel map tasks then available (yarn-) containers.
Another thing to consider when testing on big noisy clusters, is the non homogeneous runtime of nodes or node failures. Most mappers may finish long before certain others. To address for this skew in mapper runtime we suggest to set the number of mappers to a multiple (2-3 times) of available containers/threads in your cluster, reducing the runtime of a single mapper and making it cheaper to restart a task.
But be aware that a to short runtime per map tasks also hurts performance, because launching a task is associated with a considerable amount of overhead. In addition to that, more map tasks produce more intermediate files and thus causing more load for the HDFS namenode.
Try targeting run times per mapper not shorter than 3 minutes.</p>

<p>For a "small cluster" (4nodes รก 40 hardware threads) (4Nodes * 40Threads) * 2 = 320 MapTasks may be a good value.</p>

<p><strong>advanced settings</strong></p>

<p>If your cluster has more available threads then concurrently runnable containers, your cluster may be CPU underutilized.
In this case you can increase the number of threads available to the data generation tool. The data generation tool will then allocate the specified number of threads per map task.</p>

<p>Please open you Big-Bench/setEnvVars configuration file and see lines:
  export BIG_BENCH_DATAGEN_JVM_ENV=" -Xmx300m "<br>
and:<br>
  export BIGBENCH_DATAGEN_HADOOP_OPTIONS=" -workers 1 -ap 3000 "   </p>

<p>You could set
  export BIGBENCH_DATAGEN_HADOOP_OPTIONS=" -workers 4 -ap 3000 "
telling the data generation tool to use 4 threads per map task.
Note: increasing the number of threads  requires lager internal buffers so please add 100Mb of memory to  BIG_BENCH_DATAGEN_JVM_ENV per additional thread.</p>

<p>Your final settings for 4 threads per map task should look like this:
  export BIG_BENCH_DATAGEN_JVM_ENV=" -Xmx600m "
  export BIGBENCH_DATAGEN_HADOOP_OPTIONS=" -workers 4 -ap 3000 " </p>

<p>One map task per virtual CPU/hardware thread is best to utilize all CPU resources.</p>

<p>But settings in your cluster may not allow you executing this number of map tasks in parallel. Basically you can not run more parallel map tasks then available (yarn-) containers.
Another thing to consider when testing on big noisy cluster, is the non homogeneous runtime of nodes or node failures. Most mappers may finish long before certain others. To address for this skew in mapper runtime i would suggest to set the number of mappers to a multiple (2-3 times) of available containers/threads in your cluster, reducing the runtime of a single mapper and making it cheaper to restart a task.
But be aware that a to short runtime per map tasks also hurts performance. I would suggest targeting run times per mapper not shorter &gt; 5min.</p>

<h3>
<a name="hive-loading-stage-is-slow" class="anchor" href="#hive-loading-stage-is-slow"><span class="octicon octicon-link"></span></a>Hive "loading"-stage is slow,</h3>

<p>The hive loading stage is not only "moving" file in hdfs from the data/ dir into the hive/warehouse.</p>

<p>Big-Bench hive does not work on the plain CSV files, but instead transforms the files into the ORC file format, more efficient and native to hive.
Big-Bench models a set of long running analytic queries. Thus it is more realistic not to store the tables in plain text format but in a optimized fashion.</p>

<p>Transforming data into ORC is a very expensive tasks (index creation, compression, splitting and distributing/replication across the cluster) and loading the tables into hive is done by a single hive job. Since there are 23 distinct tables, hive will always create at least 23 hadoop jobs to do the CSV-&gt;ORC processing.</p>

<p>You could test if activating the following options on your cluster work for you:
hive.exec.parallel=true
hive.exec.parallel.thread.number=8</p>

<p>They allow hive to run multiple uncorrelated jobs in parallel (like creating tables). But be warned, this feature is still considered unstable (Hive 0.12).
If you cant modify your hive-site.xml cluster globally, you can uncomment/add these options in:
  BigBench/hive/hiveCreateLoadORC.sql<br>
to active them only for the loading stage or in:
  Big-Bench/hive/hiveSettings.sql
to enable them for the whole benchmark, including the queries.</p>

<h3>
<a name="hive-querys-are-running-slow" class="anchor" href="#hive-querys-are-running-slow"><span class="octicon octicon-link"></span></a>Hive Query's are running slow</h3>

<p>Unfortunately there is no generic answer to this. 
First: this is a long running benchmark with hundreds of distinct mr-Jobs. Each mr-Job has a significant amount of "scheduling" overhead of around ~1minute. So even if you are only processing no data at all, you still have to pay the price of scheduling everything (which is arround 1,5 hours! for a single wave of all 30 queries).
There are several projects trying to reduces this problem like TEZ from the stinger imitative or Hive on Spark or SparkSQL. But with Hive on yarn, there is nothing you can really do about this.</p>

<p><strong>Enough data (/bigBench -f  option) ?</strong></p>

<p>Make sure you run your benchmark with a big enough dataset to reduce the ratio of fixed overhead time vs. total runtime. Besides from initial testing your cluster setup, never run with a scaling factor of smaller than 100 ( -f 100 ==100GB)</p>

<p><strong>Enough map/reduce tasks per query stage ?</strong></p>

<p>Look in your logs and search for lines like this:</p>

<pre><code>Hadoop job information for Stage-2: number of mappers: 1; number of reducers: 1
</code></pre>

<p>If the number of mappers/reducers is &lt; than your available (yarn) slots or "tasks" you cluster is able to run (Rough estimate:  slots == number of CPU# in your cluster or: TotalClusterRAM/slotMaxMem), the query is not using all your clusters resources.
But don't generalize this. Some stages simply don't have enough data to justify more than 1 map job (e.g. the final stage of a "... limit 100 SORT BY X;" query only has to sort 100 lines).
Or the processed table is just to small (like the time or date table).
Remember that more map/reduce tasks also implies more overhead. So don't overdo it as to much map tasks can hurt performance just like to few tasks.</p>

<p>You can tune some parameters in the hive/hiveSettings.sql file. 
Hive determines the number of map/reduce tasks based on the tables size. If you have a table of 670MB and set the max.split.size to 67000000 bytes, hive will start 10 map tasks to process this table (or maybe less if hive is able to reduce the dataset by using partitioning/bucketing)</p>

<pre><code>set mapred.max.split.size=67108864;
set mapred.min.split.size=1;
set hive.exec.reducers.max=99999;
</code></pre>

<h2>
<a name="more-detailed-log-files" class="anchor" href="#more-detailed-log-files"><span class="octicon octicon-link"></span></a>More detailed log files</h2>

<p>The aggregated yarn application log file created for a yarn job contains much more information than the default printout you see on your screen.
This log file is especially helpful to debug child-processes started by hadoop MR-jobs. e.g. java/pyhton scripts in certain streaming api using queries), or  the "hadoopDataGeneration" task which executes the data generator program.</p>

<p>To retrieve this log please follow these steps:</p>

<p>In your Big-Bench/logs/ folder files or on screen you will find a line similar to this:</p>

<p>14/06/17 19:40:12 INFO impl.YarnClientImpl: Submitted application application_1403017220075_0022</p>

<p>To extract this line from the log file(s) execute:</p>

<pre><code>grep "Submitted application" ${BIG_BENCH_LOGS_DIR}/&lt;log file of interest&gt;.log
</code></pre>

<p>The important part is the application ID (e.g. application_1403017220075_0022) itself.
Take this ID and request the associate yarn log file using the following command line:</p>

<pre><code>yarn logs -applicationId &lt;applicationID&gt;  &gt; yarnApplicationLog.log
</code></pre>

<h2>
<a name="exceptionserrors-you-may-encounter" class="anchor" href="#exceptionserrors-you-may-encounter"><span class="octicon octicon-link"></span></a>Exceptions/Errors you may encounter</h2>

<h3>
<a name="execution-of-a-mr-stage-progresses-quickly-but-then-seems-to-hang-at-99" class="anchor" href="#execution-of-a-mr-stage-progresses-quickly-but-then-seems-to-hang-at-99"><span class="octicon octicon-link"></span></a>Execution of a MR Stage progresses quickly but then seems to "hang" at ~99%.</h3>

<p>This indicates a skew in the data. This means: most reducers handle only very little data, and some (1-2) have to handle most of the data.  This happens if some keys are very frequent in comparison to others. 
e.g.: this is the case for user_sk in web_clickstreams. 50% of all clicks have user_sk == NULL (indicating that the click-stream did not result in a purchase).
When a query uses the "distribute by " keyword, hive distributes the workload by this key. This implies that every reducer handles a specific set of keys. The single reducer responsible for the "null" key then effectively has to process &gt;50% of the total workload (as 50% of all keys are null).</p>

<p>We did our best to filter out null keys within the querys, if the null values are irrelevant for the query result.  This does not imply that all hive querys are "skew-free". Hive offers some settings to tune this:</p>

<pre><code>set hive.optimize.skewjoin=true;
set hive.optimize.skewjoin.compiletime=true;
set hive.groupby.skewindata=true;
set hive.skewjoin.key=100000;
-- read: https://issues.apache.org/jira/browse/HIVE-5888
</code></pre>

<p>But be aware that turning on these options will produce worse! running times for data/queries that are not heavily skewed, which is the reason they are disabled by default. </p>

<h3>
<a name="execution-failed-with-exit-status-3" class="anchor" href="#execution-failed-with-exit-status-3"><span class="octicon octicon-link"></span></a>Execution failed with exit status: 3</h3>

<pre><code>Execution failed with exit status: 3
FAILED: Execution Error, return code 3 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask
</code></pre>

<p>Hive converted a join into a locally running and faster 'mapjoin', but ran out of memory while doing so.
There are two bugs responsible for this.</p>

<p><strong>Bug 1)</strong></p>

<p>hives metric for converting joins miscalculated the required amount of memory. This is especially true for compressed files and ORC files, as hive uses the filesize as metric, but compressed tables require more memory in their uncompressed 'in memory representation'.</p>

<p>You could simply decrease 'hive.smalltable.filesize' to tune the metric, or increase 'hive.mapred.local.mem' to allow the allocation of more memory for map tasks.</p>

<p>The later option may lead to bug number two if you happen to have a affected hadoop version.</p>

<p><strong>Bug 2)</strong></p>

<p>Hive/Hadoop ignores  'hive.mapred.local.mem' !
(more exactly: bug in Hadoop 2.2 where hadoop-env.cmd sets the -xmx parameter multiple times, effectively overriding the user set hive.mapred.locla.mem setting. 
see: <a href="https://issues.apache.org/jira/browse/HADOOP-10245">https://issues.apache.org/jira/browse/HADOOP-10245</a></p>

<p><strong>There are 3 workarounds for this bug:</strong></p>

<ul>
<li>1) assign more memory to the local! Hadoop JVM client (this is not! mapred.map.memory) because map-join child jvm will inherit the parents jvm settings

<ul>
<li>In cloudera manager home, click on "hive" service,</li>
<li>then on the hive service page click on "configuration"</li>
<li>Gateway base group --(expand)--&gt; Resource Management -&gt; Client Java Heap Size in Bytes -&gt; 1GB </li>
</ul>
</li>
<li>2) reduce "hive.smalltable.filesize" to ~1MB or below (depends on your cluster settings for the local JVM)</li>
<li>3) turn off "hive.auto.convert.join" to prevent hive from converting the joins to a mapjoin.</li>
</ul><p>2) &amp; 3) can be set in Big-Bench/hive/hiveSettings.sql</p>

<h3>
<a name="cannot-allocate-memory" class="anchor" href="#cannot-allocate-memory"><span class="octicon octicon-link"></span></a>Cannot allocate memory</h3>

<pre><code>Cannot allocate memory
There is insufficient memory for the Java Runtime Environment to continue.
</code></pre>

<p>Native memory allocation (malloc) failed to allocate x bytes for committing reserved memory. </p>

<p>Basically your kernel handed out more memory than actually available, in expectants that most programs actually never use (allocate) every last bit of memory they request. Now a program (in this case java) tries to allocate something in its virtual reserved memory area, but the kernel was wrong with his estimation of application memory consumption and there is no physical memory left available to fulfil the applications malloc request.
<a href="http://www.oracle.com/technetwork/articles/servers-storage-dev/oom-killer-1911807.html">http://www.oracle.com/technetwork/articles/servers-storage-dev/oom-killer-1911807.html</a></p>

<p><strong>WARNING:</strong>
Some "fixes" suggest disabling  "vm.overcommit_memory" in the kernel.
If you are already in an "overcommitted" state DO NOT SET sysctl vm.overcommit_memory=2 on the running machine to "cure" it! If you do, you will no longer be able to execute ANY program or shell command, as this would require a memory allocation of which nothing is left. This essentially will deadlock you machine, requiring you to forcefully physically reboot the system.</p>

<h3>
<a name="javaioioexception-exceeded-max_failed_unique_fetches" class="anchor" href="#javaioioexception-exceeded-max_failed_unique_fetches"><span class="octicon octicon-link"></span></a>java.io.IOException: Exceeded MAX_FAILED_UNIQUE_FETCHES;</h3>

<pre><code>java.io.IOException: Exceeded MAX_FAILED_UNIQUE_FETCHES;
bailing-out.
</code></pre>

<p>This cryptic exception basically translates to:
Could not communicate with  node(s). Tried to copy results between nodes but
we failed after to many retries.</p>

<p>Causes:</p>

<ul>
<li>some nodes cannot communicate between each other</li>
<li>disturbed network</li>
<li>some node terminated</li>
</ul><h3>
<a name="caused-by-javalanginstantiationexception-orgapachehadoophiveqlparseastnodeorigin-" class="anchor" href="#caused-by-javalanginstantiationexception-orgapachehadoophiveqlparseastnodeorigin-"><span class="octicon octicon-link"></span></a>Caused by: java.lang.InstantiationException: org.apache.hadoop.hive.ql.parse.ASTNodeOrigin ###</h3>

<ul>
<li><a href="https://issues.apache.org/jira/browse/HIVE-6765">https://issues.apache.org/jira/browse/HIVE-6765</a></li>
<li><a href="https://issues.apache.org/jira/browse/HIVE-5068">https://issues.apache.org/jira/browse/HIVE-5068</a></li>
</ul><h3>
<a name="error-gc-overhead-limit-exceeded" class="anchor" href="#error-gc-overhead-limit-exceeded"><span class="octicon octicon-link"></span></a>Error: GC overhead limit exceeded</h3>

<pre><code>Diagnostic Messages for this Task:
Error: GC overhead limit exceeded

FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.mr.MapRedTask
</code></pre>

<p>Not enough (remote) mapper/reducer memory to complete the job.
You have to increase your mapper/reducer job memory limits (and/or yarn container limits).</p>

<p>Please read the chapter <strong>cluster setup</strong> from this FAQ section.</p>

<p>Note that this error is different from:</p>

<pre><code>Execution failed with exit status: 3
FAILED: Execution Error, return code 3 from org.apache.hadoop.hive.ql.exec.mr.MapredLocalTask
</code></pre>

<p>as "Exit status: 3" indicates a memory overflow in the "LOCAL" jvm (the jvm that started your hive task) where as "Error, return code 2" indicates a "REMOTE" problem. (A jvm started by e.g. YARN on a Node to process your job)</p>
      </section>
      <footer>
        <p>This project is maintained by <a href="https://github.com/intel-hadoop">intel-hadoop</a></p>
        <p><small>Hosted on GitHub Pages &mdash; Theme by <a href="https://github.com/orderedlist">orderedlist</a></small></p>
      </footer>
    </div>
    <script src="javascripts/scale.fix.js"></script>
    
  </body>
</html>